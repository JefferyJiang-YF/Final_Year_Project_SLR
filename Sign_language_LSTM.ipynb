{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d00ef8b",
   "metadata": {},
   "source": [
    "## 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a05f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T04:08:25.323067Z",
     "start_time": "2023-01-12T04:08:24.567024Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install --user tensorflow==2.6.0 tensorflow-gpu==2.6.0 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc7e92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:53:52.807044Z",
     "start_time": "2023-01-15T08:53:51.645005Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61cdf6",
   "metadata": {},
   "source": [
    "## 2. Keypoints using Mediapipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd32af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:53:53.744988Z",
     "start_time": "2023-01-15T08:53:53.739007Z"
    }
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #Holistic model 整体模型mp\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities 绘图工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da11ddb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:53:54.398998Z",
     "start_time": "2023-01-15T08:53:54.388036Z"
    }
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR Conversion BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                  # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR Conversion RGB 2  BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    \n",
    "     # 轮廓线 Draw face connection\n",
    "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             #mp_drawing.DrawingSpec(color=(80,110,10), thickness = 1, circle_radius = 1),\n",
    "                             #mp_drawing.DrawingSpec(color=(80,256,121), thickness = 1, circle_radius = 1))\n",
    "    \n",
    "     # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness = 2, circle_radius = 2)) \n",
    "    \n",
    "    # draw left hand connections,\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness = 2, circle_radius = 2))\n",
    "    \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness = 2, circle_radius = 2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    #如果frame中没有左手关键点就会抛出错误 注意左右手没有 res.visibility参数\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)# 压平 33 * 4\n",
    "    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b2d3",
   "metadata": {},
   "source": [
    "## 4. Setup folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4d55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:54:37.196994Z",
     "start_time": "2023-01-15T08:54:37.188023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays 制作关键点数据集文件夹\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "DATA_PATH = os.path.join('data/train') \n",
    "wlasl = ['book', 'drink', 'computer', 'before', 'chair', 'go', 'clothes', 'who', 'candy', 'cousin','deaf', 'fine', 'help', 'no', 'thin', 'walk', 'year', 'yes', 'all', 'black', 'cool', 'finish', 'hot', 'like', 'many', 'mother', 'now', 'orange', 'table', 'thanksgiving', 'what', 'woman', 'bed', 'blue', 'bowling', 'can', 'dog', 'white', 'wrong', 'accident', 'apple', 'bird', 'change', 'color', 'corn', 'cow', 'dance', 'dark', 'doctor']\n",
    "# Actions that we try to detect 只需要在这里加action即可 参考路径 data\\WLASL_train下的文件名\n",
    "actions = np.array(sorted(wlasl[0:10]))\n",
    "\n",
    "# Thirty videos worth of data\n",
    "#no_sequences = 40\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "# start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4001a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取视频数据集中每条视频帧的数量，i.e.,no_fps 并保存到 fps_list\n",
    "for action in actions:\n",
    "    fps_list = []\n",
    "    for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\Extrat_keypoints\\data\\WLASL_train\\{}\".format(action)):  # 这里就填文件夹目录就可以了\n",
    "        for file in files:\n",
    "            # 获取文件路径\n",
    "            if ('.mp4' in file):\n",
    "                path = os.path.join(root, file)\n",
    "                video = cv2.VideoCapture(path)\n",
    "                no_fps = video.get(7)\n",
    "                # video_fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "                fps_list.append(no_fps)\n",
    "        print(action, \"'s # of videos: \", len(files)) # 把这个数 可以作为no_sequences 视频数量 ！！注意必须与此Cell中#2 for 循环一起组合使用，否则len（files）数量不正确。原因是得不到正确的遍历，值只为最后一个动作的文件数总和\n",
    "        print(\"The frames that each video contains: \", fps_list,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e3844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:54:41.334034Z",
     "start_time": "2023-01-15T08:54:41.277972Z"
    }
   },
   "outputs": [],
   "source": [
    "# for action in actions:\n",
    "#     for sequence in range (no_sequences):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass\n",
    "# 制作对应手语单词的视频数量的文件夹，index 0 ~ （len - 1）\n",
    "for action in actions:\n",
    "    # NEW For loop\n",
    "    for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\Extrat_keypoints\\data\\WLASL_train\\{}\".format(action)):\n",
    "        for sequence in range (len(files)):\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f907ea",
   "metadata": {},
   "source": [
    "## 5. Collect Keypoint Values for Training and Testing|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458d91b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:05:35.941863Z",
     "start_time": "2023-01-15T08:55:39.802014Z"
    }
   },
   "outputs": [],
   "source": [
    "cap  = cv2.VideoCapture(0)\n",
    "# Set mdieapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequnece aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "        \n",
    "                # Read feed 读取喂入模型的图片\n",
    "                ret, frame = cap.read() # !!!ret 不知道是什么参数\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'START COLLECTION', (120,200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frmaes for {} video Number {}'.format(action,sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow(\"OpenCV feed\", image)                    \n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frmaes for {} video Number {}'.format(action,sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow(\"OpenCV feed\", image)   \n",
    "                    \n",
    "                # NEW export keypoints   \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully 优雅地中断\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e948b",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056be4cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:06:02.288957Z",
     "start_time": "2023-01-15T09:05:47.251110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical ##与Up主不一致的新的import方法\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55389c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:06:04.230877Z",
     "start_time": "2023-01-15T09:06:04.220911Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_data(base_data_path, actions, sequence_length):\n",
    "    label_map = {label: num for num, label in enumerate(actions)}\n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        source_folder = os.path.join(base_data_path, action)\n",
    "        leng = len(os.listdir(source_folder))\n",
    "        for sequence in range(leng):\n",
    "            window = []\n",
    "            for frame_num in range(sequence_length):\n",
    "                frame_path = os.path.join(source_folder, str(sequence), \"{}.npy\".format(frame_num))\n",
    "                res = np.load(frame_path)\n",
    "                window.append(res)\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "    print(label_map)\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train\"\n",
    "val_data_path = \"data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a517d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, train_labels = label_data(train_data_path, actions, sequence_length)\n",
    "val_sequences, val_labels = label_data(val_data_path, actions, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to_categorical(labels).astype(int) on labels\n",
    "train_labels_categorical = to_categorical(train_labels).astype(int)\n",
    "val_labels_categorical = to_categorical(val_labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(train_sequences),train_labels_categorical\n",
    "X_val, y_val = np.array(val_sequences), val_labels_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b73198",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35120426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:07:04.827943Z",
     "start_time": "2023-01-15T09:07:04.814988Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential # allow us to build a sequential neural network\n",
    "from tensorflow.python.keras.layers import LSTM, Dense,Dropout # LSTM: temoporal component, Dense: a normal fully connected layer\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint # allow us to perform some logging inside, trace and monitor our model as its training\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648111ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:07:06.339016Z",
     "start_time": "2023-01-15T09:07:06.239069Z"
    }
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "log_dir = os.path.join('Logs', \"LSTM\",now.strftime(\"%Y-%m-%d-%H-%M-L2\"))\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "print(now.strftime(\"%Y-%m-%d-%H-%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53395f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:07:10.431039Z",
     "start_time": "2023-01-15T09:07:09.499971Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropout_rate = 0.5\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 258))) # 64 LSTM units not layers 30 frames with 1662 param\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(dropout_rate))\n",
    "# model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "reg_strength = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 258), kernel_regularizer=l2(reg_strength))) # 64 LSTM units not layers 30 frames with 1662 param\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', kernel_regularizer=l2(reg_strength)))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu', kernel_regularizer=l2(reg_strength)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(reg_strength)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(reg_strength)))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(reg_strength)))\n",
    "model.add(Dense(actions.shape[0], activation='softmax', kernel_regularizer=l2(reg_strength)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc22f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:07:20.027889Z",
     "start_time": "2023-01-15T09:07:19.994002Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',  \n",
    "loss='categorical_crossentropy', \n",
    "metrics = ['accuracy']) #多类别的loss使用中间的，二分的类别使用binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the ModelCheckpoint callback\n",
    "checkpoint_filepath = (\"models/LSTM/best_model_weights.h5\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb5588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:15:36.389020Z",
     "start_time": "2023-01-15T09:07:28.159038Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    callbacks = [tb_callback],\n",
    "    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab299f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:51:58.867225Z",
     "start_time": "2023-01-12T08:51:58.857258Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05134d",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f33fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:18:23.276153Z",
     "start_time": "2023-01-15T09:18:22.891000Z"
    }
   },
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ca17f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:18:26.762002Z",
     "start_time": "2023-01-15T09:18:26.749088Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce420c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:18:29.186038Z",
     "start_time": "2023-01-15T09:18:29.177068Z"
    }
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61474e21",
   "metadata": {},
   "source": [
    "## 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe10f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:57:36.403310Z",
     "start_time": "2023-01-12T08:57:36.360222Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('action_220_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76ac20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:58:39.742207Z",
     "start_time": "2023-01-12T08:58:39.736226Z"
    }
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f68c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:59:30.091317Z",
     "start_time": "2023-01-12T08:59:30.064297Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('action_220_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589a532",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a97dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:00:14.626139Z",
     "start_time": "2023-01-12T09:00:14.613183Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f02c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:06:39.943160Z",
     "start_time": "2023-01-12T09:06:39.832313Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bab9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:06:41.959068Z",
     "start_time": "2023-01-12T09:06:41.942255Z"
    }
   },
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a15c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:06:44.310691Z",
     "start_time": "2023-01-12T09:06:44.301478Z"
    }
   },
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881e297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:07:02.999281Z",
     "start_time": "2023-01-12T09:07:02.993302Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84eacf",
   "metadata": {},
   "source": [
    "## 11 . Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11de749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T09:18:38.917901Z",
     "start_time": "2023-01-15T09:18:38.898966Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3964e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T07:27:28.516382Z",
     "start_time": "2023-01-22T07:27:28.267420Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.95\n",
    "\n",
    "cap  = cv2.VideoCapture(0)\n",
    "# Set mdieapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        \n",
    "        # Read feed 读取喂入模型的图片\n",
    "        ret, frame = cap.read() # !!!ret 不知道是什么参数\n",
    "        \n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow(\"OpenCV feed\", image)\n",
    "\n",
    "        # Break gracefully 优雅地中断\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
