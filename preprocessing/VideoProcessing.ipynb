{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c5ae15",
   "metadata": {},
   "source": [
    "# 1. Import and install needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8a4d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T07:47:20.899481Z",
     "start_time": "2023-01-13T07:47:17.468457Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install --user tensorflow==2.6.0 tensorflow-gpu==2.6.0 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a68861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T09:19:22.704312Z",
     "start_time": "2023-01-13T09:19:22.690359Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db0abf",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcff51d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T09:19:24.032070Z",
     "start_time": "2023-01-13T09:19:24.020110Z"
    }
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh]) #without face\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    \n",
    "     # 轮廓线 Draw face connection\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness = 1, circle_radius = 1),\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness = 1, circle_radius = 1))\n",
    "    \n",
    "     # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness = 2, circle_radius = 2)) \n",
    "    \n",
    "    # draw left hand connections,\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness = 2, circle_radius = 2))\n",
    "    \n",
    "    # draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness = 2, circle_radius = 4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness = 2, circle_radius = 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cced438",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vedio capture (Used for collecting sign language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61756eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T08:08:03.178353Z",
     "start_time": "2023-01-13T08:07:47.530416Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已保存 record_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# 定义逐帧处理函数，可不进行任何处理，直接将摄像头捕获的画面写入视频帧 （代关键点的保存）\n",
    "# 使用前置Webcam 收集数据集脚本，一次一条\n",
    "output_name = 'record_video.mp4'\n",
    "\n",
    "# 获取摄像头，传入0表示获取系统默认摄像头\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 打开cap\n",
    "cap.open(0)\n",
    "\n",
    "frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "out = cv2.VideoWriter(output_name, fourcc, fps, (int(frame_size[0]), int(frame_size[1])))\n",
    "\n",
    "# 无限循环，直到break被触发\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # 获取画面\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        ## 将帧写入视频文件中\n",
    "        out.write(image)\n",
    "\n",
    "        # 展示处理后的三通道图像\n",
    "        cv2.imshow('press q to break', image)\n",
    "\n",
    "        if cv2.waitKey(1) in [ord('q'),27]: # 按键盘上的q或esc退出（在英文输入法下）\n",
    "            break\n",
    "\n",
    "    # 关闭图像窗口\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    # 关闭摄像头\n",
    "    cap.release()\n",
    "\n",
    "    print('视频已保存', output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadd7f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T08:20:39.195407Z",
     "start_time": "2023-01-13T08:20:33.077336Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频已保存 record_video_1.mp4\n"
     ]
    }
   ],
   "source": [
    "# 调用摄像头拍摄视频模板 （无关键点保存）\n",
    "# 生成的视频文件名默认为output_video.mp4，帧处理函数process_frame()默认不进行任何处理\n",
    "# 同济子豪兄 2021-7-11\n",
    "\n",
    "# 导入opencv-python\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# 定义逐帧处理函数，可不进行任何处理，直接将摄像头捕获的画面写入视频帧\n",
    "def process_frame(img):\n",
    "    return img\n",
    "\n",
    "output_name = 'record_video_1.mp4'\n",
    "\n",
    "# 获取摄像头，传入0表示获取系统默认摄像头\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 打开cap\n",
    "cap.open(0)\n",
    "\n",
    "frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "out = cv2.VideoWriter(output_name, fourcc, fps, (int(frame_size[0]), int(frame_size[1])))\n",
    "\n",
    "# 无限循环，直到break被触发\n",
    "while cap.isOpened():\n",
    "    # 获取画面\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # 对捕获的帧进行图像处理\n",
    "    frame = process_frame(frame)\n",
    "    \n",
    "    ## 将帧写入视频文件中\n",
    "    out.write(frame)\n",
    "    \n",
    "    # 展示处理后的三通道图像\n",
    "    cv2.imshow('press q to break', frame)\n",
    "\n",
    "    if cv2.waitKey(1) in [ord('q'),27]: # 按键盘上的q或esc退出（在英文输入法下）\n",
    "        break\n",
    "    \n",
    "# 关闭图像窗口\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "out.release()\n",
    "\n",
    "# 关闭摄像头\n",
    "cap.release()\n",
    "\n",
    "print('视频已保存', output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88fcd7",
   "metadata": {},
   "source": [
    "# 4. Load the video that without landmarks and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bdd05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.0, 31.0, 73.0, 18.0, 18.0, 28.0, 33.0, 69.0, 90.0, 99.0, 79.0, 93.0, 104.0, 75.0, 82.0, 98.0, 80.0, 93.0, 100.0, 80.0, 87.0, 77.0, 95.0, 75.0, 78.0, 85.0, 88.0, 86.0, 83.0, 63.0, 124.0, 74.0, 64.0, 33.0, 47.0, 96.0, 124.0, 64.0, 100.0, 121.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 获取视频数据集中每条视频帧的数量，i.e.,no_fps 并保存到 fps_list\n",
    "fps_list = []\n",
    "for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\Extrat_keypoints\\data\\WLASL_train\\book\"):  # 这里就填文件夹目录就可以了\n",
    "    for file in files:\n",
    "        # 获取文件路径\n",
    "        if ('.mp4' in file):\n",
    "            path = os.path.join(root, file)\n",
    "            video = cv2.VideoCapture(path)\n",
    "            no_fps = video.get(7)\n",
    "            video_fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "            fps_list.append(no_fps)\n",
    "    print(fps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94404f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'all', 'apple', 'bed', 'bird', 'black', 'blue', 'bowling', 'can', 'change', 'color', 'cool', 'corn', 'cow', 'dance', 'dark', 'deaf', 'doctor', 'dog', 'f', 'fine', 'finish', 'help', 'hot', 'like', 'many', 'mother', 'no', 'now', 'orange', 'table', 'thanksgiving', 'thin', 'walk', 'what', 'white', 'woman', 'wrong', 'year', 'yes']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "wlasl = ['book', 'drink', 'computer', 'before', 'chair', 'go', 'clothes', 'who', 'candy', 'cousin', 'deaf', 'fine', 'help', 'no', 'thin', 'walk', 'year', 'yes', 'all', 'black', 'cool', 'finish', 'hot', 'like', 'many', 'mother', 'now', 'orange', 'table', 'thanksgiving', 'what', 'woman', 'bed', 'blue', 'bowling', 'can', 'dog', 'f', 'white', 'wrong', 'accident', 'apple', 'bird', 'change', 'color', 'corn', 'cow', 'dance', 'dark', 'doctor', 'eat', 'enjoy', 'forget', 'give', 'last', 'meet', 'pink', 'pizza', 'play', 'school', 'secretary', 'short', 'time', 'want', 'work', 'africa', 'basketball', 'birthday', 'brown', 'but', 'cheat', 'city', 'cook', 'decide', 'full', 'how', 'jacket', 'letter', 'medicine', 'need', 'paint', 'paper', 'pull', 'purple', 'right', 'same', 'son', 'tell', 'thursday']\n",
    "wlasl_10 = ['book', 'drink', 'computer', 'before', 'chair', 'go', 'clothes', 'who', 'candy', 'cousin']#1-10\n",
    "wlasl_40 = ['deaf', 'fine', 'help', 'no', 'thin', 'walk', 'year', 'yes', 'all', 'black', 'cool', 'finish', 'hot', 'like', 'many', 'mother', 'now', 'orange', 'table', 'thanksgiving', 'what', 'woman', 'bed', 'blue', 'bowling', 'can', 'dog', 'f', 'white', 'wrong', 'accident', 'apple', 'bird', 'change', 'color', 'corn', 'cow', 'dance', 'dark', 'doctor']# 11-50\n",
    "wlasl_40.sort()\n",
    "print(wlasl_40)\n",
    "print(len(wlasl_40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e640a4bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T09:19:41.019439Z",
     "start_time": "2023-01-13T09:19:41.008477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays 制作关键点数据集文件夹\n",
    "DATA_PATH = os.path.join('test') \n",
    "\n",
    "# Actions that we try to detect 只需要在这里加action即可 参考路径 data\\WLASL_train下的文件名\n",
    "actions = np.array(wlasl_10)\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 40\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "# sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "# start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afa90183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-13T09:00:14.054797Z",
     "start_time": "2023-01-13T09:00:13.994001Z"
    }
   },
   "outputs": [],
   "source": [
    "# 制作对应手语单词的视频数量的文件夹，index 0 ~ （len - 1）\n",
    "for action in actions:\n",
    "    for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\fyp_LSTM\\ActionDetectionforSignLanguage\\data\\test\\{}\".format(action)):\n",
    "        for sequence in range (len(files)):\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f2f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 获取视频数据集中每条视频帧的数量，i.e.,no_fps 并保存到 fps_list\n",
    "for action in actions:\n",
    "    fps_list = []\n",
    "    for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\fyp_LSTM\\ActionDetectionforSignLanguage\\data\\test\\{}\".format(action)):  # 这里就填文件夹目录就可以了\n",
    "        for file in files:\n",
    "            # 获取文件路径\n",
    "            if ('.mp4' in file):\n",
    "                path = os.path.join(root, file)\n",
    "                video = cv2.VideoCapture(path)\n",
    "                no_fps = video.get(7)\n",
    "                # video_fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "                fps_list.append(no_fps)\n",
    "        print(action, \"'s # of videos: \", len(files)) # 把这个数 可以作为no_sequences 视频数量 ！！注意必须与此Cell中#2 for 循环一起组合使用，否则len（files）数量不正确。原因是得不到正确的遍历，值只为最后一个动作的文件数总和\n",
    "        print(\"The frames that each video contains: \", fps_list,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba730a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(files.index(file)))\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ab8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for root, dirs, files in os.walk(r\"C:\\deep-learning\\HKMU\\fyp_LSTM\\ActionDetectionforSignLanguage\\data\\test\\{}\".format(action)):  # 这里就填文件夹目录就可以了\n",
    "            for file in files:\n",
    "                print()\n",
    "                if ('.mp4' in file):\n",
    "                    path = os.path.join(root, file)\n",
    "                    cap = cv2.VideoCapture(path)\n",
    "                    no_fps = int(cap.get(7))\n",
    "                    #print(action, files.index(file), no_fps)\n",
    "                    for frame_num in range(no_fps):\n",
    "                        \n",
    "                        ret, frame = cap.read()\n",
    "                        image, results = mediapipe_detection(frame, holistic)\n",
    "                        draw_styled_landmarks(image, results)\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        npy_path = os.path.join(DATA_PATH, action, str(files.index(file)), str(frame_num))\n",
    "                        np.save(npy_path, keypoints)\n",
    "                        if files.index(file) == len(files):\n",
    "                            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21c150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keypoints_data_folders(data_path, actions):\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(data_path, action)\n",
    "        if not os.path.exists(action_path):\n",
    "            os.makedirs(action_path)\n",
    "        for root, dirs, files in os.walk(os.path.join(data_path, action)):\n",
    "            for sequence in range(len(files)):\n",
    "                sequence_path = os.path.join(action_path, str(sequence))\n",
    "                if not os.path.exists(sequence_path):\n",
    "                    os.makedirs(sequence_path)\n",
    "                    \n",
    "def process_videos(base_path, actions, data_path):\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for action in actions:\n",
    "            for root, dirs, files in os.walk(os.path.join(base_path, action)):\n",
    "                for file in files:\n",
    "                    if '.mp4' in file:\n",
    "                        path = os.path.join(root, file)\n",
    "                        cap = cv2.VideoCapture(path)\n",
    "                        no_fps = int(cap.get(7))\n",
    "\n",
    "                        for frame_num in range(no_fps):\n",
    "                            ret, frame = cap.read()\n",
    "                            image, results = mediapipe_detection(frame, holistic)\n",
    "                            draw_styled_landmarks(image, results)\n",
    "                            keypoints = extract_keypoints(results)\n",
    "                            npy_path = os.path.join(data_path, action, str(files.index(file)), str(frame_num))\n",
    "                            \n",
    "                            os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                            np.save(npy_path, keypoints)\n",
    "                            if files.index(file) == len(files):\n",
    "                                break\n",
    "\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fea6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for raw datasets\n",
    "raw_train_path = \"../rawdata/train\"\n",
    "raw_val_path = \"../rawdata/val\"\n",
    "raw_test_path = \"../rawdata/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753729a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for keypoints datasets\n",
    "DATA_PATH_TRAIN = \"keypoints/train\"\n",
    "DATA_PATH_VAL = \"keypoints/val\"\n",
    "DATA_PATH_TEST = \"keypoints/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a793cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for keypoints data\n",
    "create_keypoints_data_folders(DATA_PATH_TRAIN, wlasl_40)\n",
    "create_keypoints_data_folders(DATA_PATH_VAL, wlasl_40)\n",
    "create_keypoints_data_folders(DATA_PATH_TEST, wlasl_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938f65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process train, validation, and test videos\n",
    "process_videos(raw_train_path, wlasl_40, DATA_PATH_TRAIN)\n",
    "process_videos(raw_val_path, wlasl_40, DATA_PATH_VAL)\n",
    "process_videos(raw_test_path, wlasl_40, DATA_PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b506a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8cee8606174e3625038b6e7ec9be7f93f6c6df4ccc079b3e52670062f23e6199"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
